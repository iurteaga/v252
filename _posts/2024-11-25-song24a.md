---
title: Bidirectional Generative Pre-training for Improving Healthcare Time-series
  Representation Learning
abstract: Learning time-series representations for discriminative tasks, such as classification
  and regression, has been a long-standing challenge in the healthcare domain. Current
  pre-training methods are limited in either unidirectional next-token prediction
  or randomly masked token prediction. We propose a novel architecture called Bidirectional
  Timely Generative Pre-trained Transformer (BiTimelyGPT), which pre-trains on biosignals
  and longitudinal clinical records by both next-token and previous-token prediction
  in alternating transformer layers. This pre-training task preserves original distribution
  and data shapes of the time-series. Additionally, the full-rank forward and backward
  attention matrices exhibit more expressive representation capabilities. Using biosignals
  and longitudinal clinical records, BiTimelyGPT demonstrates superior performance
  in predicting neurological functionality, disease diagnosis, and physiological signs.
  By visualizing the attention heatmap, we observe that the pre-trained BiTimelyGPT
  can identify discriminative segments from biosignal time-series sequences, even
  more so after fine-tuning on the task.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: song24a
month: 0
tex_title: Bidirectional Generative Pre-training for Improving Healthcare Time-series
  Representation Learning
cycles: false
bibtex_author: Song, Ziyang and Lu, Qincheng and Zhu, He and Buckeridge, David L.
  and Li, Yue
author:
- given: Ziyang
  family: Song
- given: Qincheng
  family: Lu
- given: He
  family: Zhu
- given: David L.
  family: Buckeridge
- given: Yue
  family: Li
date: 2024-11-25
address:
container-title: Proceedings of the 9th Machine Learning for Healthcare Conference
volume: '252'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 11
  - 25
pdf: https://raw.githubusercontent.com/mlresearch/v252/main/assets/song24a/song24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
